<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Einops Arena: Shape Fluency · Arena Workbook</title>
  <style>
    :root {
      --bg: #0f172a;
      --panel: #111827;
      --border: #1f2937;
      --accent: #22d3ee;
      --muted: #94a3b8;
      --text: #e2e8f0;
      --pill: #1d4ed8;
      --pill-text: #e0f2fe;
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: "Inter", "SF Pro Display", "Segoe UI", sans-serif;
      background: radial-gradient(120% 80% at 10% 10%, rgba(34,211,238,0.12), rgba(34,211,238,0) 55%), var(--bg);
      color: var(--text);
      line-height: 1.6;
      padding: 32px 24px 64px;
    }
    .page {
      max-width: 1100px;
      margin: 0 auto;
    }
    h1 {
      font-size: 32px;
      margin: 0 0 8px 0;
      letter-spacing: -0.02em;
    }
    .summary {
      color: var(--muted);
      max-width: 780px;
      margin-bottom: 16px;
    }
    .grid {
      display: grid;
      grid-template-columns: 1fr 320px;
      gap: 20px;
    }
    @media (max-width: 1024px) {
      .grid { grid-template-columns: 1fr; }
    }
    .panel {
      background: linear-gradient(145deg, rgba(17,24,39,0.9), rgba(15,23,42,0.92));
      border: 1px solid var(--border);
      border-radius: 14px;
      padding: 16px 18px;
      box-shadow: 0 20px 60px rgba(0,0,0,0.35);
    }
    .section-title {
      font-size: 14px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: var(--muted);
      margin-bottom: 10px;
    }
    ul.goals {
      list-style: disc;
      padding-left: 18px;
      margin: 0;
      color: var(--text);
    }
    .pill {
      display: inline-block;
      padding: 6px 10px;
      border-radius: 999px;
      background: var(--pill);
      color: var(--pill-text);
      font-size: 12px;
      margin: 4px 6px 4px 0;
    }
    .resources ul {
      padding-left: 16px;
      margin: 0;
    }
    .resources a {
      color: var(--accent);
      text-decoration: none;
    }
    .resources a:hover { text-decoration: underline; }
    .exercise {
      margin-bottom: 14px;
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 14px 16px;
      background: linear-gradient(145deg, rgba(30,41,59,0.75), rgba(15,23,42,0.9));
    }
    .exercise h3 {
      margin: 0 0 6px 0;
      font-size: 18px;
      letter-spacing: -0.01em;
    }
    .ex-meta {
      color: var(--muted);
      font-size: 13px;
      margin-bottom: 8px;
    }
    .objective {
      margin: 8px 0;
    }
    pre {
      background: #0b1223;
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 12px;
      overflow-x: auto;
      font-size: 13px;
      line-height: 1.5;
      color: #cbd5e1;
      margin-top: 10px;
    }
    .list {
      padding-left: 16px;
      margin: 6px 0;
      color: var(--text);
    }
    .list li { margin: 2px 0; }
    .badges {
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
      margin-top: 8px;
    }
    .note { color: var(--muted); font-size: 13px; margin-top: 8px; }
  </style>
</head>
<body>
  <main class="page">
    <header>
      <h1>Einops Arena: Shape Fluency</h1>
      <p class="summary">A focused arena of exercises to build muscle memory for rearrange, reduce, and einsum in real model settings.</p>
    </header>
    <section class="grid">
      <div>
        <article class="exercise">
  <div class="ex-meta">Exercise 1 · patches · Kata name: einops_patches</div>
  <h3>Patchify images with rearrange</h3>
  <p class="objective">Turn BCHW images into non-overlapping flattened patches.</p>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Acceptance</div>
  <ul class="list"><li>Output shape is (batch, num_patches, channels * patch_area)</li><li>Patch order does not interleave rows or channels</li><li>patch_size must divide both height and width</li></ul>
</div>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Hints</div>
  <ul class="list"><li>Use both spatial axes in the einops pattern</li><li>Keep channels leading when flattening patches</li></ul>
</div>
  
  
  <div class="note">Add later via TUI: Library → All Katas → "einops_patches" (or press w for Workbooks, then a/p).</div>
  <pre><code>&quot;&quot;&quot;Convert images into flattened patches with einops.&quot;&quot;&quot;

import torch
from einops import rearrange
from jaxtyping import Float


def image_to_patches(
    images: Float[torch.Tensor, &quot;batch channels height width&quot;],
    patch_size: int,
) -&gt; Float[torch.Tensor, &quot;batch num_patches patch_dim&quot;]:
    &quot;&quot;&quot;Split images into non-overlapping flattened patches.

    Args:
        images: input tensor shaped (batch, channels, height, width).
        patch_size: edge length of each square patch. Both height and width
            must be divisible by patch_size.

    Returns:
        Tensor of shape (batch, num_patches, channels * patch_size * patch_size).
    &quot;&quot;&quot;
    assert patch_size &gt; 0
    batch, channels, height, width = images.shape
    assert height % patch_size == 0
    assert width % patch_size == 0

    # BLANK_START
    ...
    # BLANK_END</code></pre>
</article>
<article class="exercise">
  <div class="ex-meta">Exercise 2 · segment-mean · Kata name: einops_segment_mean</div>
  <h3>Sequence pooling with reduce</h3>
  <p class="objective">Average tokens over fixed windows without losing batch or feature dimensions.</p>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Acceptance</div>
  <ul class="list"><li>Sequence length must be divisible by window_size</li><li>Output shape is (batch, segments, dim) with the correct mean per window</li></ul>
</div>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Hints</div>
  <ul class="list"><li>Group the sequence axis as (segments window) in the pattern</li></ul>
</div>
  
  <div class="badges"><span class="pill">patches</span></div>
  <div class="note">Add later via TUI: Library → All Katas → "einops_segment_mean" (or press w for Workbooks, then a/p).</div>
  <pre><code>&quot;&quot;&quot;Segment-wise mean pooling with einops.reduce.&quot;&quot;&quot;

import torch
from einops import reduce
from jaxtyping import Float


def segment_mean(
    tokens: Float[torch.Tensor, &quot;batch seq dim&quot;],
    window_size: int,
) -&gt; Float[torch.Tensor, &quot;batch segments dim&quot;]:
    &quot;&quot;&quot;Compute mean over non-overlapping windows along sequence.&quot;&quot;&quot;
    assert window_size &gt; 0
    batch, seq, _ = tokens.shape
    assert seq % window_size == 0

    # BLANK_START
    ...
    # BLANK_END</code></pre>
</article>
<article class="exercise">
  <div class="ex-meta">Exercise 3 · split-heads · Kata name: einops_split_heads</div>
  <h3>Unmerge attention heads</h3>
  <p class="objective">Split a merged hidden dimension into (heads, head_dim) for attention.</p>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Acceptance</div>
  <ul class="list"><li>hidden_dim must be divisible by num_heads</li><li>Output shape is (batch, heads, seq, head_dim) with contiguous per-head blocks</li></ul>
</div>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Hints</div>
  <ul class="list"><li>Do not reorder batch or sequence axes</li><li>Use a named dimension for heads in the pattern</li></ul>
</div>
  
  <div class="badges"><span class="pill">segment-mean</span></div>
  <div class="note">Add later via TUI: Library → All Katas → "einops_split_heads" (or press w for Workbooks, then a/p).</div>
  <pre><code>&quot;&quot;&quot;Split merged attention heads with einops.&quot;&quot;&quot;

import torch
from einops import rearrange
from jaxtyping import Float


def split_heads(
    hidden: Float[torch.Tensor, &quot;batch seq hidden_dim&quot;],
    num_heads: int,
) -&gt; Float[torch.Tensor, &quot;batch heads seq head_dim&quot;]:
    &quot;&quot;&quot;Reshape merged heads into (heads, head_dim).&quot;&quot;&quot;
    assert num_heads &gt; 0
    _, _, hidden_dim = hidden.shape
    assert hidden_dim % num_heads == 0

    # BLANK_START
    ...
    # BLANK_END</code></pre>
</article>
<article class="exercise">
  <div class="ex-meta">Exercise 4 · attention-logits · Kata name: einops_attention_logits</div>
  <h3>Scaled attention logits with einsum</h3>
  <p class="objective">Compute QK^T / sqrt(d_model) using einops.einsum across batch and heads.</p>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Acceptance</div>
  <ul class="list"><li>Output shape is (batch, heads, q_len, k_len)</li><li>Uses sqrt(dim) scaling (no hardcoded constant)</li><li>Matches torch.einsum baseline for the same inputs</li></ul>
</div>
  <div>
  <div class="section-title" style="margin-bottom:4px;">Hints</div>
  <ul class="list"><li>Validate head counts and dimensions before computing logits</li><li>Keep batch and head axes aligned in the pattern</li></ul>
</div>
  
  <div class="badges"><span class="pill">split-heads</span></div>
  <div class="note">Add later via TUI: Library → All Katas → "einops_attention_logits" (or press w for Workbooks, then a/p).</div>
  <pre><code>&quot;&quot;&quot;Scaled dot-product attention logits with einops.&quot;&quot;&quot;

import math

import torch
from einops import einsum
from jaxtyping import Float


def scaled_attention_logits(
    query: Float[torch.Tensor, &quot;batch heads q_len dim&quot;],
    key: Float[torch.Tensor, &quot;batch heads k_len dim&quot;],
) -&gt; Float[torch.Tensor, &quot;batch heads q_len k_len&quot;]:
    &quot;&quot;&quot;Compute attention logits with sqrt(dim) scaling.&quot;&quot;&quot;
    assert query.shape[0] == key.shape[0]
    assert query.shape[1] == key.shape[1]
    assert query.shape[3] == key.shape[3]

    # BLANK_START
    ...
    # BLANK_END</code></pre>
</article>
      </div>
      <div class="panel">
        <div class="section-title">Learning goals</div>
        <ul class="goals"><li>Read and rewrite tensor shapes without guessing</li><li>Flatten and unflatten image patches with channel-first order intact</li><li>Pool and reshape sequences using einops.reduce instead of ad-hoc slicing</li><li>Compute attention logits with einops.einsum and correct scaling</li></ul>
        <div class="section-title" style="margin-top:14px;">Prerequisites</div>
        <div><span class="pill">tensor_slicing</span><span class="pill">softmax</span></div>
        <div class="section-title" style="margin-top:14px;">Resources</div>
        <div class="resources">
          <ul><li><a href="https://einops.rocks/pytorch-examples.html" target="_blank" rel="noreferrer noopener">einops overview</a></li><li><a href="https://einops.rocks/notes/cheatsheet/" target="_blank" rel="noreferrer noopener">einops cheatsheet</a></li></ul>
        </div>
      </div>
    </section>
  </main>
</body>
</html>
