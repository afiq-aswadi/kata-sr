<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Attention</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #333;
        }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 0.5rem; }
        h2 { color: #34495e; margin-top: 2rem; }
        code { background-color: #f4f4f4; padding: 0.2rem 0.4rem; border-radius: 3px; }
        pre { background-color: #f4f4f4; padding: 1rem; border-radius: 5px; overflow-x: auto; }
        .key-point { background-color: #d1ecf1; padding: 1rem; border-left: 4px solid #17a2b8; margin: 1rem 0; }
    </style>
</head>
<body>
    <h1>Multi-Head Attention</h1>

    <div class="key-point">
        <strong>Idea:</strong> Instead of performing a single attention function, multi-head attention runs multiple attention operations in parallel, each with different learned linear transformations.
    </div>

    <h2>Why Multiple Heads?</h2>
    <p>
        Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.
    </p>

    <h2>The Formula</h2>
    <pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O

where head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)</code></pre>

    <p>
        Each head learns different patterns:
        <ul>
            <li>Head 1 might focus on syntactic relationships</li>
            <li>Head 2 might focus on semantic relationships</li>
            <li>Head 3 might focus on positional relationships</li>
        </ul>
    </p>

    <h2>Implementation Details</h2>
    <p>
        <strong>Typical Configuration:</strong>
        <ul>
            <li>Number of heads (h): 8 or 16</li>
            <li>Model dimension (d_model): 512</li>
            <li>Per-head dimension: d_model / h = 64</li>
        </ul>
    </p>

    <p>
        The total computational cost is similar to single-head attention with full dimensionality, but multi-head provides richer representations.
    </p>
</body>
</html>
