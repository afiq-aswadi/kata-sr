<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What is Attention?</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #333;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 0.5rem;
        }
        h2 {
            color: #34495e;
            margin-top: 2rem;
        }
        code {
            background-color: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 1rem;
            border-left: 4px solid #ffc107;
            margin: 1rem 0;
        }
        .key-point {
            background-color: #d1ecf1;
            padding: 1rem;
            border-left: 4px solid #17a2b8;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <h1>What is Attention?</h1>

    <div class="key-point">
        <strong>Key Concept:</strong> Attention mechanisms allow neural networks to focus on specific parts of the input when producing each element of the output.
    </div>

    <h2>The Intuition</h2>
    <p>
        Imagine you're reading a sentence to translate it into another language. You don't process all words simultaneously with equal importance. Instead, you <strong>pay attention</strong> to specific words that are most relevant for translating the current word.
    </p>

    <p>
        For example, when translating "The cat sat on the mat" to French:
        <ul>
            <li>For "cat" → "chat", you focus on the word "cat"</li>
            <li>For "sat" → "s'est assis", you focus on both "cat" and "sat"</li>
            <li>For "mat" → "tapis", you focus on "the" and "mat"</li>
        </ul>
    </p>

    <h2>Why Attention Matters</h2>
    <p>
        Before attention mechanisms, sequence-to-sequence models (like RNNs) had to compress the entire input sequence into a single fixed-size vector. This created a bottleneck, especially for long sequences.
    </p>

    <div class="highlight">
        <strong>Problem:</strong> How can a single vector capture all the nuances of a long sentence or paragraph?
    </div>

    <p>
        Attention solves this by allowing the model to:
        <ol>
            <li><strong>Look back</strong> at the entire input sequence at each decoding step</li>
            <li><strong>Compute relevance scores</strong> for each input element</li>
            <li><strong>Create a weighted summary</strong> that focuses on the most important parts</li>
        </ol>
    </p>

    <h2>The Attention Formula (Preview)</h2>
    <p>
        At its core, attention computes three things:
    </p>
    <ul>
        <li><strong>Query (Q):</strong> What am I looking for?</li>
        <li><strong>Key (K):</strong> What does each input represent?</li>
        <li><strong>Value (V):</strong> What information does each input contain?</li>
    </ul>

    <p>
        We'll dive deeper into the mathematics in the next section!
    </p>

    <div class="key-point">
        <strong>Next Up:</strong> Learn how to compute attention scores using the scaled dot-product formula.
    </div>
</body>
</html>
