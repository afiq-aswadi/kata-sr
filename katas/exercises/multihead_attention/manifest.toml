[kata]
name = "multihead_attention"
tags = ["transformers", "deprecated", "composite"]
base_difficulty = 5
description = """
[DEPRECATED - Use micro-katas instead]

This kata has been split into focused micro-exercises:
1. attention_qk_similarity - compute Q @ K^T / sqrt(d_k)
2. attention_weights - apply softmax to scores
3. attention_values - weight and sum values
4. attention_multihead - combine into multi-head attention

This composite kata combines all building blocks for advanced practice.

Key concepts: attention mechanism, multi-head parallelism, scaling factor
"""
dependencies = ["attention_multihead"]

[[variations]]
name = "attention_causal"
description = "Add causal masking for autoregressive generation"
params = { mask_type = "causal" }
