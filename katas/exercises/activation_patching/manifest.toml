[kata]
name = "activation_patching"
category = "interpretability"
base_difficulty = 4
description = """
Implement activation patching for causal intervention in language models.

Patch activations from one input (clean) into another (corrupted) to identify
which components causally matter. Core technique for circuit discovery and
mechanistic interpretability.

Key concepts: causal intervention, activation patching, circuit analysis, counterfactuals
"""
dependencies = ["transformerlens_extract_residual", "transformerlens_run_with_cache"]
